{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63c56cb-9be6-40ec-b2e4-1186e2aae351",
   "metadata": {},
   "source": [
    "## <span style= 'color: rebeccapurple;'> CODE FOR SCRAPING DATA FROM BUSINESS DAY NEWS ARTICLE </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e4b2b3-8933-4580-9433-c717c0bf3257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7f6aa-5fda-4834-9049-a00514ce2172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from rake_nltk import Rake\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import time\n",
    "import os\n",
    "\n",
    "# === Download required NLTK resources ===\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# === STEP 1: Load URLs from Excel ===\n",
    "input_excel = \"MY_BUSINESSDAY_ARTICLE.xlsx\"\n",
    "\n",
    "if not os.path.exists(input_excel):\n",
    "    raise FileNotFoundError(f\"âŒ File not found: {input_excel}\")\n",
    "\n",
    "df_urls = pd.read_excel(input_excel)\n",
    "\n",
    "# === STEP 2: Set headers and sentiment analyzer ===\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "results = []\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(f\"ðŸš€ Starting scrape on {len(df_urls)} article URLs...\\n\")\n",
    "\n",
    "# === STEP 3: Scrape and analyze each article ===\n",
    "for idx, row in df_urls.iterrows():\n",
    "    url = row.get(\"url\") or row.get(\"URL\")\n",
    "    if not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "        continue\n",
    "\n",
    "    # âœ… Remove AMP version if present\n",
    "    url = url.replace(\"?amp\", \"\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # === Extract Title ===\n",
    "        title_tag = soup.find(class_=\"post-title\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else row.get(\"title\", \"\")\n",
    "\n",
    "        # === Robust Author Extraction ===\n",
    "        author = \"\"\n",
    "\n",
    "        # Try CSS selector first\n",
    "        author_tag = soup.select_one(\"span.post-author a\")\n",
    "        if author_tag:\n",
    "            author = author_tag.get_text(strip=True)\n",
    "\n",
    "        # Fallback: meta tag\n",
    "        if not author:\n",
    "            author_meta = soup.find(\"meta\", attrs={\"name\": \"author\"})\n",
    "            if author_meta and author_meta.has_attr(\"content\"):\n",
    "                author = author_meta[\"content\"].strip()\n",
    "\n",
    "        if not author:\n",
    "            print(f\"[DEBUG] Author not found on: {url}\")\n",
    "\n",
    "        # === Extract Published Date ===\n",
    "        date_tag = soup.find(class_=\"post-date\")\n",
    "        pub_date = date_tag.get_text(strip=True) if date_tag else row.get(\"published_date\", \"\")\n",
    "\n",
    "        # === Extract Content ===\n",
    "        content_tag = soup.find(class_=\"post-content\")\n",
    "        content = content_tag.get_text(separator=\" \", strip=True) if content_tag else \"\"\n",
    "\n",
    "        # === Sentiment Analysis using VADER ===\n",
    "        sentiment_scores = analyzer.polarity_scores(content)\n",
    "        polarity = sentiment_scores['compound']\n",
    "        sentiment = (\n",
    "            \"Positive\" if polarity >= 0.05 else\n",
    "            \"Negative\" if polarity <= -0.05 else\n",
    "            \"Neutral\"\n",
    "        )\n",
    "\n",
    "        # === Simple Summary: First 5 sentences ===\n",
    "        sentences = nltk.sent_tokenize(content)\n",
    "        summary = \" \".join(sentences[:5])\n",
    "\n",
    "        # === Keyword Extraction using RAKE ===\n",
    "        try:\n",
    "            rake = Rake()\n",
    "            rake.extract_keywords_from_text(content)\n",
    "            keyword_list = rake.get_ranked_phrases()[:10]\n",
    "            top_keywords = \", \".join(keyword_list)\n",
    "        except:\n",
    "            top_keywords = \"\"\n",
    "\n",
    "        # === Save result ===\n",
    "        results.append({\n",
    "            \"URL\": url,\n",
    "            \"Title\": title,\n",
    "            \"Author\": author,\n",
    "            \"Published Date\": pub_date,\n",
    "            \"Content\": content,\n",
    "            \"Summary\": summary,\n",
    "            \"Keywords\": top_keywords,\n",
    "            \"Sentiment\": sentiment,\n",
    "            \"Polarity Score\": polarity\n",
    "        })\n",
    "\n",
    "        print(f\"{idx+1}/{len(df_urls)} âœ… {title[:60]}...\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{idx+1}/{len(df_urls)} âŒ Error scraping {url}: {e}\")\n",
    "        continue\n",
    "\n",
    "# === STEP 4: Save results to CSV ===\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_csv = f\"businessday_articles_vader_{timestamp}.csv\"\n",
    "pd.DataFrame(results).to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Done! All data saved to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1f38efc-afeb-4f51-8b16-fec0407dd116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gracious\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gracious\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gracious\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Gracious\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Gracious\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All done! Cleaned data with new sentiment saved to: updated_scraped_cleaned_with_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# === NLTK Downloads ===\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# === Load Existing CSV ===\n",
    "input_csv = \"updated_scraped.csv\"  \n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# === Preprocessing Setup ===\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()  # Ensure string and lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word in tokens\n",
    "        if word not in stop_words and word.isalpha()\n",
    "    ]\n",
    "    return \" \".join(clean_tokens)\n",
    "\n",
    "# === Apply Preprocessing ===\n",
    "df[\"Cleaned Content\"] = df[\"Content\"].astype(str).apply(preprocess_text)\n",
    "\n",
    "# === Re-run VADER Sentiment Analysis ===\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    polarity = scores['compound']\n",
    "    sentiment = (\n",
    "        \"Positive\" if polarity >= 0.05 else\n",
    "        \"Negative\" if polarity <= -0.05 else\n",
    "        \"Neutral\"\n",
    "    )\n",
    "    return pd.Series([sentiment, polarity])\n",
    "\n",
    "df[[\"Cleaned Sentiment\", \"Cleaned Polarity\"]] = df[\"Cleaned Content\"].apply(analyze_sentiment)\n",
    "\n",
    "# === Save to New CSV ===\n",
    "output_csv = input_csv.replace(\".csv\", \"_cleaned_with_sentiment.csv\")\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"âœ… All done! Cleaned data with new sentiment saved to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bee43b-9531-428c-807f-9e03bb8e9e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
